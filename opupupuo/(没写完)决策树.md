## 决策树训练与预测

### 决策树概念

**决策树**是一种树形结构，通过特征的不同来将样本数据划分到不同的分支（子树）中，最终，每个样本 一定会划分到一个叶子节点中。我们可以将每个特征视为一个问题（提问），特征值的不同，就视为样 本给出的不同答案，然后，我们就可以根据一系列问题（特征），将样本划分到不同的叶子节点中。决 策树可以用于分类与回归任务。

### 训练决策树
给定如下数据集：
![[Pasted image 20230602212805.png]]
可以通过三个特征作为三个问题，依次”询问“数据集中的每个样本。结果如下图所示。
![[Pasted image 20230604163850.png]]

### 预测原理
-------------
我们可以想象，假设对A与B两个人进行性格测试，两个人回答一些相同的选择题，如果两个人的选项 完全一致，则说明两个人的性格存在很大的相似性。同样，决策树是根据特征值来划分样本的（这类似 于回答问题）。如果样本经过层层划分之后，分到了同一个叶子节点中，则表明这些样本应该也是非常 相似的。因此，我们就可以使用在同一个叶子节点中的已知样本，去预测未知样本的标签了。

预测的方式为：
+ 对于分类树，使用叶子节点中，数量最多的类别，作为未知样本的类别。
+ 对于回归树，使用叶子节点中，所有样本标签（$y$）均值，作为位置样本的输出值（$\widehat y$）。

## 决策树特征选择

从最直观的角度，以分类任务为例：
+ 哪个特征（取值）能够将样本类别划分的效果好，就选哪个特征（取值）


## 信息熵
-------------
### 概念

信息熵，在1948年由香农提出。用来衡量信息的不确定性或随机性。不确定性越大，则信息熵越大，反 之，信息熵越小。 例如，4只猎豹参与赛跑，每只猎豹的能力都是旗鼓相当，平分秋色。我们很难确定哪只猎豹会获得胜 利，因此，这种情况下，不确定性很大，信息熵就大。但是，假设让1只猎豹与3只蜗牛进行赛跑，则猎 豹取胜便是毋容置疑的，因此，这种情况下，不确定性很小，信息熵就小。

### 计算方式
假设随机变量X具有m个值，分别为$V_1 , V_2 , ....V_m$ 。并且各个值出现概率如下：

$$
\left\{\begin{array}{c}P(X=V_1)=p_1\\ P(X=V_2)=p_2\\ P(X=V_3)=p_3\\ \cdots\\ P(X=V_m)=p_m\end{array}\right.
$$

并且：
$$p_1+p_2+p_3+....+p_m=1$$

则变量X的信息熵为
$$\begin{aligned}
&H(X)=-p_1*log_2p_1-p_2*log_2p_2-\cdots-p_m*log_2p_m \\
&=-\sum_{i=1}^m p_i\log_2 p_i
\end{aligned}$$
### 不纯度

从数据集的角度将，信息熵是样本**不纯度**的度量。
+ 样本中的类别越均衡，则不纯度越大，信息熵越大。
+ 样本中类别失衡，则不纯度小，信息熵也小。

比如![[Pasted image 20230604165934.png]]
上述题目中，c选项的比例均衡，不纯度就高。

## 信息增益
**信息增益**（IG-Information gain）定义如下：$$
IG(D_p,f)=I(D_p)-\sum_{j=1}^n\frac{N_j}{N_p}I(D_j)
$$
+ $f$：划分的特征。
+ $D_p$：父节点，即使用$f$分割之前的节点。
+ $IG(D_p,f)$：父节$D_p$点使用特征$f$的划分下，获取的信息增益
+ $D_j$：父节点$D_p$经过分割后，会产生n个子节点，$D_j$是第j个节点
+ $N_p$：父节点$D_p$包含的样本数量。
+ $N_j$：第j个子节点$D_j$包含岩本的数量。
+ $I$：不纯度度量标准。

出于简化与缩小组合搜索空间的考虑，很多库（包括scikit-learn）实现的都是二叉决策树，即每个父节 点最多含有两个子节点（左子树节点与右子树节点），此时，信息增益定义为：$$
IG(D_p,f)=I(D_p)-\frac{N_{left}}{N_p}I(D_{left})-\frac{N_{right}}{N_p}I(D_{right})
$$

通过定义我们可知，信息增益就是父节点的不纯度减去所有子节点不纯度（加权）。

<font color="05B3F8" size="5">信息增益的作用是筛选出划分后信息熵最小的那个特征（取值），信息增益最大的特征作为优先划分的特征。</font>

## 训练规则

在sklearn中，训练分类决策树的具体规则如下：
1. 所有特征均为数值类型。
    + 对于类别变量，可以转换为离散值。
    + 每个特征，会设定一个阈值，小于等于阈值，划分到左子树，否则划分到右子树。
2. 从根节点开始，选择可获得最大信息增益的特征进行分裂（实现信息增益最大化）。
3. 对子节点继续选择能够获得最大信息增益的特征进行分裂，直到满足如下条件之一，停止分裂。
    + 所有叶子节点中的样本属于同一个类别。
    + 树达到指定的最大深度（max_depth），每次分裂视为一层。
    + 节点包含的样本数量小于指定的最小分裂样本数量（min_samples_split）。
    + 如果节点分裂后，叶子节点包含的样本数量小于指定的叶子最小样本数量 （min_samples_leaf）。

## 不纯度度量标准
------
不纯度可以采用如下方式度量：
+ 信息熵（Enriopy）
+ 基尼系数（Gini Index）
+ 错误率（classification error）

### 信息熵
$$
I_H(D)=-\sum_{i=1}^mp(i\mid D)log_2p(i\mid D)
$$
+ $m$：节点$D$中含有样本的类别数量
+ $p(i\mid D)$：节点D中，属于类别i的样本占节点D中样本总数的比例（概率）

### 基尼系数
$$
I_G(D)=1-\sum_{i=1}^m p(i\mid D)^2
$$
<font color="05B3F8" size="4.9">在决策树中，基尼系数衡量的是特征的纯度或不纯度。</font>

### 错误率
$$
I_E(D)=1-max\{p(i\mid D)\}
$$

## 决策树算法

决策树主要包含以下三种算法：
+ ID3
+ C4.5
+ CART (Classification And Regression Tree)

### ID3
ID3（Iterative Dichotomiser3-迭代二分法）算法是非常经典的决策树算法，该算法描述如下： 
+ 使用多叉树结构。 
+ 使用信息熵作为不纯度度量标准，选择信息增益最大（信息熵最小）的特征分割数据。 

ID3算法简单，训练较快。但该算法具有一些局限，如下： 
+ 不支持连续特征。 (如年收入)
+ 不支持缺失值。
+ 仅支持分类，不支持回归。 
+ 在选择特征时，会倾向于选择类别多的特征。（简单的说就是当特征特别多的时候，它会无脑选类别多的那个特征）

### C4.5
C4.5算法是在ID3算法上改进而来，该算法描述如下： 
+ 使用多叉树结构。 
+ 仅支持分类，不支持回归。

不过，C4.5在ID3算法上，进行了一些优化，包括： 
+ 支持对缺失值的处理。 
+ 支持将连续值进行离散化处理。 
+ 使用信息熵作为不纯度度量标准，但选择<font color="FFA488" size="5.5">信息增益率（而不是信息增益）最大的特征</font>分裂节点。当特征变多的时候，信息熵（分母）会变大，这个算法随之变小，所以不会像ID3那样无脑选类别多的那个特征。
信息增益率的定义方式为：$$
IG_{Ratio}(D_p,f)=\frac{IG_H(D_p,f)}{I_H(f)}
$$
+ $I_H(f)$：根据特征$f$的不同类别比例（概率），计算得到的信息熵。

之所以从信息增益改为信息增益率，是因为在ID3算法中，倾向于选择类别多的特征，因此，经过这样 的调整，在C4.5中就可以得到缓解。因为类别多的特征在计算信息熵$I_H(f)$时，往往会比类别少的特征信息熵大。这样，就可以在分母上进行一定的惩罚。

![[Pasted image 20230605084256.png]]