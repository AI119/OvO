+ 偏差和方差理解
+ Bagging算法原理
+ 随机森林算法原理

## 集成方法介绍
---------
集成方法（集成学习）是一种解决问题的思想。操作为将若干个基本评估器(模型)进行组合，然后使用 这些基本评估器来综合对未知样本进行预测。通过这种“集思广益”的行为，比起使用单个基本评估器， 集成方法通常具有更好的泛化能力或稳健性。

## 偏差与方差

### 偏差

偏差（Bias）是指模型在训练集上的表现与真实值之间的差异。 
+ 如果模型的偏差很高，说明模型无法很好地拟合训练数据，可能出现欠拟合的情况。 
+ 偏差高的模型通常较简单，可能需要增加模型的复杂度来提高拟合能力。

[[高偏差的算法]]

### 方差

方差（Variance）是指模型在不同训练集上预测结果的变化程度。 
+ 如果模型的方差很高，那么说明模型对训练数据过度拟合，可能出现过拟合的情况。 
+ 方差高的模型通常较复杂，可能需要减少模型的复杂度来提高泛化能力。

偏差和方差之间存在着一种权衡关系，称为**偏差-方差权衡（Bias-Variance Tradeoff）**。在机器学习中，我们通常追求找到一个模型的最佳复杂度，使其在偏差和方差之间取得平衡，以获得最好的泛化能力。

[[高方差的算法]]

## 集成方法分类
------
集成方法可以分为以下两类： 
+ 平均方法。 
+ 提升方法。

### 平均方法

平均方法（averaging methods）训练多个独立的基本评估器（评估器之间没有关联），然后对多个评 估器的预测结果进行平均化。
+ 对于分类任务，使用每个评估器预测结果中，类别数量（或平均概率）最多的那个类别作为预测结 果。 
+ 对于回归任务，使用多个评估器预测结果的均值作为预测结果。 
平均方法通过综合考量的行为，<font color="05B3F8" size="5.4">可以有效的减少方差</font>，因此，其预测结果通常可以优于任何一个基本评估器。平均方法是并行完成任务

[[平均方法的算法]]包括[[Bagging]]，[[随机森林(Random Forest)]]，[[Voting]]
，[[Stacking]]
### 提升方法

在提升方法（boosting methods）中，多个基本评估器是按顺序训练的，然后将这些基本评估器（通 常是弱评估器）进行组合，进而产生一个预测能力强的评估器。与平均方法不同，提升方法的多个基本 评估器不是独立的，后续评估器需要依赖于之前的评估器，训练过程中，会试图减少组合之后（整体评 估器）的<font color="05B3F8" size="5.4">偏差</font>。
提升方法是串行完成任务

## 集成方法效果

以二分类为例，如果存在$n$个分类器，每个分类器的错误率都为$e$且各个分类器之间是独立的。因此，多个分类器集成之后的错误率服从二项分布，其中，$k$个分类器出错的概率可表示为：$$
P(y=k)=C_n^k e^k(1-e)^{n-k}
$$
假设现有11个分类器，单个分类器的错误率为0.25，则如果集成分类器出错，则至少需要6个（或6个以 上）的分类器出错，集成后分类器出错的概率为：$$
P(y\geqslant k)=\sum_{k=6}^n C_{11}^k0.25^k*0.75^{11-k}=0.034
$$
注意：<font color="05B3F8" size="5.4">集成评估器效果不一定好于基本</font>

## [[Bagging]]

Bagging算法采用平均方法的思想，也称为汇聚法(Bootstrap Aggregating)。算法过程如下： 
1. 在原始数据集上进行随机抽样（抽样可以是放回抽样与不放回抽样）。 
2. 使用得到的随机子集来训练评估器（基本评估器）。 
3. 重复步骤1与步骤2若干次，会得到n个基本评估器。 
4. 将 个评估器进行组合，根据多数投票（分类）或者求均值（回归）的方式来统计最终的结果（平均方法）。

### 优势

bagging方法通过随机抽样来构建原始数据集的子集，来训练不同的基本评估器，然后再将多个基本评 估器进行组合来预测结果，这样可以有效减小基本评估器的方差。因此，通过bagging方法，就可以非 常便捷的对基本评估器进行改进，而无需去修改基本评估器底层的实现。 
因为bagging方法可以有效的降低过拟合，因此，bagging方法适用于强大而复杂的模型（例如，完全 生长的决策树）。

### 缺点
1. **高计算成本：** Bagging 需要构建多个独立的基学习器，每个学习器都要使用自助采样生成一个与原始数据集相同大小的训练集。这导致了计算成本的增加，尤其是在数据集规模较大时。
    
2. **降低模型解释性：** Bagging 通过组合多个基学习器的预测结果来获得最终预测，这增加了整体模型的复杂性。由于模型是通过平均方法进行集成的，最终的预测结果不再具有明确的解释性。因此，如果需要对模型的预测进行解释，Bagging 可能不是最佳选择。
    
3. **可能对噪声敏感：** 在自助采样的过程中，Bagging 可能会引入一些噪声样本。这些噪声样本可能对模型产生不利影响，特别是对于噪声样本的标签错误或异常样本的情况。在这种情况下，Bagging 可能无法有效减少模型的方差，从而导致集成模型的性能下降。
    
4. **有限多样性：** Bagging 通过自助采样增加了基学习器之间的差异性，但仍然存在一定的相关性。如果基学习器之间过于相似，集成模型的性能可能无法得到明显改善。这种情况下，可以尝试引入更多的随机性，例如随机特征选择或随机子空间方法。
    
5. **对类别不平衡问题处理困难：** 当处理类别不平衡的数据集时，Bagging 可能会对较少类别的样本预测效果不佳。由于自助采样的性质，会导致某些类别在训练集中出现较少的情况，从而影响模型对少数类别的学习效果。在这种情况下，可以尝试使用平衡采样方法或其他针对类别不平衡问题的技术。

## 随机森林(Random Forest)

随机森林(Random Forest)是一种元评估器，内部集成多棵决策树，并使用平均方法来计算预测结果。 其实现为： 
1. 如果是放回抽样，则从原始数据集中选出 `max_samples` 个样本用于训练，否则使用所有原始数据 集样本。 
2. 使用这些样本来构建一棵决策树。 从所有特征中随机选择 `max_features` 个特征（特征不重复）。 根据 `criterion` 参数所指定的准则（信息熵或基尼系数），选择选定的特征对节点进行划 分。 
3. 重复以上两步 `n_estimators` 次，即建立 `n_estimators` 棵决策树。 
4. 这 `n_estimators` 棵决策树形成随机森林，通过平均概率（分类）或均值（回归）决定最终的预测值。

关于随机森林，具有如下的说明： 
+ 随机森林的基本评估器，固定为决策树。 
+ 因为随机森林就是组合多棵决策树，因此，决策树的很多参数，也适用于随机森林。 
+ 由于这种随机性，随机森林的偏差通常会略微增加（相对于单棵决策树），但由于使用多棵决策树 平均预测，其方差也会减小，从而从整体上来讲，模型更加优秀。
+ 在分类预测时，scikit-learn中使用平均概率来预测结果，而不是加1进行投票。 
+ 对于回归任务，通常设置 `max_features=1.0` 或 `max_features=None` （使用所有特征），对于分 类任务，通常设置 `max_features="sqrt"` 。

### 极度随机树（Extremely Randomized Trees）

极度随机树在实现上与随机森林类似，不过更加随机。二者的区别在于：
- 生成阈值的方式不同：
	- 对于随机森林，会在候选特征（`max_features`）中，选择最好的阈值作为划分规则。
	- 对于极度随机树，会为每个候选特征随机生成划分阈值，然后在这些随机阈值上，选择最好的一个特征作为划分规则
- 选择训练样本的默认值不同：
	- 随机森林默认采用放回抽样选择样本，即`bootstrap=True`。
	- 极度随机森林默认使用原始数据集，即`bootstrap=False`
极度随机树引进了更大的随机性，这通常可以进一步降低模型的方差，不过，偏差也会略有增加，原因是<big>阈值的选择是随机的，所以会导致拟合能力会稍微弱一些，容易导致错误判断</big>。

### 分类程序示例
``` python
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
X, y = make_classification(n_samples=2000, n_features=2, n_informative=2,
n_redundant=0, n_classes=2, random_state=0,
flip_y=0.2)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=0)

tree = DecisionTreeClassifier(criterion="gini")
tree = tree.fit(X_train, y_train)
print("决策树分类正确率：")
print(tree.score(X_train, y_train))
print(tree.score(X_test, y_test))

#n_estimators:基本评估器（决策树）的数量
#max_samples:每次抽样用于训练基本评估器的样本数量。
#bootstrap:如果为True（默认值），抽样允许重复（放回抽样）。如果为False，则使用所有的样本训练没课决策树。
# 注意该参数的含义与bagging的参数有所不同。
# max_samples：当bootstrap为True时，训练每棵决策树所使用的样本数。默认为None（使用所有的样本）。
rf = RandomForestClassifier(n_estimators=100, criterion="gini",
random_state=0, bootstrap=True, max_samples=0.9, max_features="sqrt")
rf.fit(X_train, y_train)
print("随机森林正确率：")
print(rf.score(X_train, y_train))
print(rf.score(X_test, y_test))

et = ExtraTreesClassifier(n_estimators=100, criterion="gini",
random_state=0, bootstrap=True,
max_samples=0.9, max_features="sqrt")
et.fit(X_train, y_train)
print("极度随机树正确率：")
print(et.score(X_train, y_train))
print(et.score(X_test, y_test))
```
上述结果[[绘图_1]]

### 特征重要度

在随机森林中，同样提供feature_importances_属性来返回每个特征的重要度。

数据集导入、展示
``` python
from sklearn.datasets import load_digits digits = load_digits() 
X, y = digits.data, digits.target 
# 只取0与6的图像。 
mask = (y == 0) | (y == 6) 
X = X[mask] 
y = y[mask] 
# 图像为8 * 8。保存在长度为64的数组中。 
print(X.shape, y.shape) 
row, col = 5, 4 
fig, ax = plt.subplots(row, col) 
ax = ax.ravel() 
fig.set_size_inches(15, row * 5) 
for i in range(row * col): 
	ax[i].imshow(X[i].reshape(8, 8), cmap="gray")
```

切分数据集并测试
``` python
rf = RandomForestClassifier(n_estimators=100, max_features=0.8, random_state=0, bootstrap=True, n_jobs=-1) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=80) 
rf.fit(X_train, y_train) 
print(rf.score(X_train, y_train)) 
print(rf.score(X_test, y_test))
```

绘制特征重要度
``` python
# 返回特征的重要度。特征的重要度根据该特征对目标变量（y）的可预测性来度量。 
# 特征对目标变量的可预测性越有帮助，则重要度越大，否则越小。 
importances = rf.feature_importances_ print(importances) 
# 特征重要度的权重之和为1。 
print(np.sum(importances)) importances = importances.reshape(8, 8) 
# 绘制特征的重要度。 
plt.matshow(importances, cmap=plt.cm.hot) 
plt.colorbar()
```
![[Pasted image 20230605210445.png]]

## Voting
-------------------------
Voting表示投票，表决，其内部集成多个评估器，通过每个评估器（预测结果）表决的方式来决定 Voting的预测结果。

### 分类
---------
当使用Voting分类时，包括以下两种类型： 
+ 硬投票：根据每个基本评估器预测类别的数量来决定最终预测类别。 
+ 软投票：根据基本评估器的平均概率来决定最终预测类别。 
同时，我们也可以通过 weights 来指定每个基本评估器的权重，默认为等权重。

``` python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier

X, y = make_classification(n_samples=5000, n_features=10, n_informative=3,
n_classes=3, random_state=0, flip_y=0.3)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,
random_state=0)
knn = KNeighborsClassifier(n_neighbors=5)
lr = LogisticRegression()
tree = DecisionTreeClassifier()
e = [("knn", knn), ("lr", lr), ("tree", tree)]
# estimators：指定所有集成的评估器，格式与Pipeline类的steps参数类似。
vote = VotingClassifier(estimators=e)
for name, model in e + [("voting", vote)]:
    print(name)
    model.fit(X_train, y_train)
    print(model.score(X_test, y_test))
```

``` python
from sklearn.model_selection import GridSearchCV
params = {
"knn__n_neighbors": [5, 8, 11],
"knn__weights": ["uniform", "distance"],
"lr__C": [0.01, 0.1, 1, 10],
"tree__max_depth": [5, 7, 9, None],
"voting": ["hard", "soft"],
"weights": [[1, 1, 1]]
}
grid = GridSearchCV(estimator=vote, param_grid=params, cv=5, n_jobs=-1,
scoring="accuracy", verbose=10)
grid = grid.fit(X_train, y_train)
print(grid.best_params_)
print(grid.best_estimator_.score(X_test, y_test))
```

### 回归

``` python
from sklearn.ensemble import VotingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_friedman1

#生成一个非线性的数据集。
X, y = make_friedman1(n_samples=5000, n_features=10, random_state=123,
noise=2)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=2332)
knn = KNeighborsRegressor(n_neighbors=5)
lr = LinearRegression()
tree = DecisionTreeRegressor()
e = [("knn", knn), ("lr", lr), ("tree", tree)]
vote = VotingRegressor(estimators=e)
for name, model in e + [("voting", vote)]:
    print(name)
    model.fit(X_train, y_train)
    print(model.score(X_test, y_test))
```

``` python
params = {
"knn__n_neighbors": [5, 8, 11],
"knn__weights": ["uniform", "distance"],
"tree__max_depth": [5, 7, 9, None],
"weights": [[1, 1, 1]]
}
grid = GridSearchCV(estimator=vote, param_grid=params, cv=5, n_jobs=-1,
scoring="r2", verbose=10)
grid = grid.fit(X_train, y_train)
print(grid.best_params_)
print(grid.best_estimator_.score(X_test, y_test))
```








## Stacking
-------
堆叠，是一种集成学习技术，在结构上分为基础模型与最终模型（元模型），主要思想是使用基本模型 的输出作为最终模型的输入特征。 
堆叠的过程如下： 
+ 基础模型训练：使用训练集训练多个基础模型。 
+ 基础模型预测：使用训练好的基础模型，在训练集上预测结果。 
+ 最终模型训练：将基础模型的预测结果，作为元模型的输入特征，训练元模型。 
+ 最终模型预测：对新数据（未知样本）进行预测，首先将数据传递给基础模型生成预测。 然后， 将基础模型的预测用作最终模型的输入特征，产生最终预测结果。 

优点： 
+ 可以通过结合不同基础模型的优势来提高整体预测性能。 
+ 可以通过使用交叉验证和聚合来自多个模型的预测来降低过度拟合的风险。 
缺点：
+ 在计算上较昂贵，因为需要训练多个基础模型和一个最终模型。
+ 堆叠模型的性能取决于基础模型和最终模型的质量。 如果基础模型较弱或最终模型不适合任务， 则堆叠模型可能表现不佳。
+ 与bagging或boosting等相对简单的集成技术相比，堆叠的实现和理解可能更复杂。

``` python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier
X, y = make_classification(n_samples=5000, n_features=10, n_informative=3,
n_classes=3, random_state=0, flip_y=0.3)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,
random_state=0)
knn = KNeighborsClassifier(n_neighbors=5)
lr = LogisticRegression()
tree = DecisionTreeClassifier()
rf = RandomForestClassifier(n_estimators=200)
e = [("knn", knn), ("lr", lr), ("tree", tree)]
# estimators：基本评估器。
# final_estimator：最终评估器。
stack = StackingClassifier(estimators=e, final_estimator=rf, n_jobs=-1,
cv=5)
for name, model in e + [("rf", rf), ("stack", stack)]:
    print(name)
    model.fit(X_train, y_train)
    print(model.score(X_test, y_test))
```
![[Pasted image 20230606112414.png]]

