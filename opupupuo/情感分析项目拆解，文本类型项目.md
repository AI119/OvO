-------
情感分析是文本分类中常见的应用场景。简单来说，就是从一段文本描述中，理解文本的感情色彩。常 见的情感分析就是客户对商品或者服务的反馈，例如顾客对商品，酒店的评价等场景。在传统模式下， 往往是通过人工对文本内容进行核对，从而将文本分为褒义，中性，贬义等。这种方式会消耗大量的人 力资源，并且效率不高。

## 任务与实现
-------
我们以分析电影评论为例。我们的任务在于，根据电影评论中的内容，进行文本预处理，建模等操作， 从而可以识别评论的感情色彩，节省人力资源。 
具体实现内容包括： 
+ 能够对文本数据进行预处理。 
	- 文本清洗。 
	- 分词，
	- 去除停用词。
	- 文本向量化。 
+ 能够通过Python统计词频，生成词云图。 
+ 能够通过方差分析，进行特征选择。 
+ 能够根据文本向量，对文本数据进行分类。

## 加载数据
导入相关的库
``` python
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
sns.set(style="darkgrid") 
plt.rcParams["font.family"] = "SimHei" 
plt.rcParams["axes.unicode_minus"] = False
```

``` python
# 读取数据，昵称列对我们分析没有意义，直接去掉。
data = pd.read_csv("comment.csv", usecols=["time", "city", "gender", "level", "score", "comment"]) 
print(data.shape) 
display(data.head())
```

## 数据预处理
-----
### 文本数据
------
**结构化数据与非结构化数据**

结构化数据，是可以表示成多行多列的形式，并且，每行（列）都有着具体的含义。非结构化数据，无法合理的表 示为多行多列的形式，即使那样表示，每行（列）也没有具体的含义。

**文本数据预处理**
文本数据，是一种非结构化数据。因此，其预处理的步骤与方式也会与结构化数据有所差异。文本数据预处理主要 包含：
+ 缺失值处理 
+ 重复值处理 
+ 文本内容清洗 
+ 分词 
+ 停用词处理

### **缺失值处理**
--------
检测缺失值并处理
``` python
data.isnull().sum()
```
发现包含缺失值的列![[Pasted image 20230529161723.png]]
``` python
data["city"].fillna("未知", inplace=True) 
# 删除没有评论的数据。 
data.dropna(inplace=True) 
data.isnull().sum()
```

### 重复值处理
重复的数据，对文本分析与建模没有帮助，我们可以直接删除重复记录。
查询重复记录
``` python
print(data.duplicated().sum()) 
display(data[data.duplicated()].iloc[:5])
```

删除重复记录
``` python
data.drop_duplicates(inplace=True) 
print(data.duplicated().sum())
```

### 文本内容清洗
使用正则表达式匹配标点符号。这里需要注意与 `re.sub()` 方法不同的是，`re_obj.sub()` 方法是通过预先编译的正则表达式对象调用的。这种预编译可以提高处理大量文本时的效率，因为编译只需要在第一次执行时进行，后续的调用可以直接使用编译好的正则表达式对象进行匹配和替换操作。
``` python
import re 
re_obj = re.compile( 
	r"[!\"#$%&'()*+,-./:;<=>?@[\\\]^_`{|}~—！，。？、￥…（）：【】《》‘’“”\s]+") 
def clear(text): 
	return re_obj.sub("", text) 
	
data["comment"] = data["comment"].apply(clear) 
data.head()
```

### 分词
------
分词是将连续的文本，分割成语义合理的若干词汇序列。对于英文来说，分词是非常容易的，但是中文分词会有一 定的难度。我们可以通过jieba来实现分词的功能。
这里需要注意的是，`jieba.cut(text)`返回的是一个迭代器，每次迭代产生一个分词结果字符串。
``` python
def cut_word(text): 
	return jieba.cut(text) 
data["comment"] = data["comment"].apply(cut_word) 
data.sample(5)
```

### 停用词处理
------------
停用词，指的是在我们语句中大量出现，但却对语义分析没有帮助的词。对于这样的词汇，我们通常可以将其删 除，这样的好处在于：
1. 可以降低存储空间消耗。 
2. 可以减少计算时间消耗。
``` python
def get_stopword(): 
	s = set() 
	with open("stopword.txt", encoding="UTF-8") as f: 
	for line in f: 
		s.add(line.strip()) 
	return s 

def remove_stopword(words): 
	return [word for word in words if word not in stopword] 

stopword = get_stopword() 
data["comment"] = data["comment"].apply(remove_stopword) 
data.sample(5)
```

## 数据基本分析
----------
### 用户所在城市
-----------------
统计在所包含的用户中，数量最多与最少的10个城市。
``` python
fig, ax = plt.subplots(1, 2) 
fig.set_size_inches(12, 4) 
count = data["city"].value_counts() 
top = count.iloc[:10] 
bottom = count.iloc[-10:] 
for index, d, title in zip(range(2), [top, bottom], ["前十城市", "后十城市"]): 
	a = sns.barplot(x=d.index, y=d.values, ax=ax[index]) 
	# 旋转45度，避免字体重叠。 
	a.set_xticklabels(a.get_xticklabels(), rotation=60) 
	for container in a.containers: 
		a.bar_label(container) 
	a.set_title(title)
```

### 用户等级
---------------
绘制用户每个等级的数量。
``` python
a = sns.countplot(x="level", data=data, log=True) 
for container in a.containers: 
	a.bar_label(container)
```
![[Pasted image 20230529203726.png]]
注意，生成图形时使用了log参数，为了避免数值相差很大时，显示效果较差。

### 绘制评分
``` python
a = sns.countplot(x="score", data=data, log=True) 
for container in a.containers: 
	a.bar_label(container)
```
![[Pasted image 20230529204320.png]]
由上图可知好评数量较多，将评分分为三个等级，绘制直方图。 
评分 
+ 好评4.5-5 
+ 中评3-4
+ 差评0-2.5

``` python
data_period = [0, 2.51, 4.01, 5] 
# 也可以使用matplotlib来绘制直方图。 
# plt.hist(data["score"], bins=data_period, log=True) 
# bins指定的区间为左闭右开，最后一个区间为双闭。 
a = sns.histplot(data["score"], bins=data_period) 
# seaborn的histplot方法没有设置log的参数，需要通过axes自行设置。 
a.set_yscale("log") 
# 让x轴显示的数值与bins指定的区间一致。 
a.set_xticks(data_period) 
for container in a.containers: 
	a.bar_label(container)
```
![[Pasted image 20230529210122.png]]

### 性别对比
--------------
统计评论用户中，性别的对比。
``` python
# 0：未知，1：男，2：女。 
a = sns.countplot(x="gender", data=data) 
for container in a.containers: 
	a.bar_label(container)
```
![[Pasted image 20230529210442.png]]

### 评论时间分布
----------------
提取用户写评论的时间段
``` python
# 时间列的格式：2019-07-31 21:21:02 
# extract抽取用户活跃的时间段，正则表达式\d提取数字，{2}提取两个连续的，加上冒号是
# 因为要匹配两个连续的尾部有冒号的数字，这个函数自动返回匹配到的第一个符合要求的数值，
# expand=False：不展开抽取的结果，如果为True，会把抽取出的每一个字符都展开为单独的一列
hour = data["time"].str.extract(r" (\d{2}):", expand=False) 
hour = hour.astype(np.int32) 
# 直方图的区间的前闭后开，最后一个区间双闭。 
time_period = [0, 6, 12, 18, 24] 
a = sns.histplot(hour, bins=time_period) 
a.set_xticks(time_period) 
for container in a.containers: 
	a.bar_label(container)
```
![[Pasted image 20230530110132.png]]

## 词汇统计

----------

### [[词汇频数统计]]

统计在所有评论中，出现频数最多的N个词汇
``` python
from itertools import chain

from collections import Counter
  

li_2d = data["comment"].tolist()

# 将二维列表扁平化为一维列表。

li_1d = list(chain.from_iterable(li_2d))

print(f"总词汇量：{len(li_1d)}")

c = Counter(li_1d)

print(f"不重复词汇数量：{len(c)}")

common = c.most_common(15)

print(common)
```

将统计出来的词汇进行可视化
``` python
d = dict(common)

plt.figure(figsize=(12, 5))

# seaborn内部不支持dict_keys与dict_values类型，maplotlib.bar可以。

a = sns.barplot(x=list(d.keys()), y=list(d.values()))

for container in a.containers:

    a.bar_label(container)
```

![[Pasted image 20230530110849.png]]

``` python
#将出现次数除以总数即可得出相应频数

total = len(li_1d)

percentage = [v * 100 / total for v in d.values()]

#{v:2f}%取小数后两位

print([f"{v:.2f}%" for v in percentage])

plt.figure(figsize=(12, 5))

a = sns.barplot(x=list(d.keys()), y=percentage)

for container in a.containers:

    a.bar_label(container, fmt="%.2f")
```

![[Pasted image 20230530111044.png]]

### 频数分布统计

绘制所有词汇的频数分布直方图。
``` python
plt.figure(figsize=(12, 5)) v = list(c.values()) 
# 最大的词频数，对应的log值。 
end = np.log10(max(v)) 
# 根据在10^0 ~ 10^end之间，生成等比数列，作为直方图的区间（bins）。 
a = sns.histplot(v, bins=np.logspace(0, end, num=10)) 
for container in a.containers: 
	a.bar_label(container) 
a.set_xscale("log") 
a.set_yscale("log")
```

![[Pasted image 20230530114144.png]]

### 评论词汇长度统计

统计每个用户评论使用的词汇数量。并绘制用词最多的前$N$个评论。

``` python
plt.figure(figsize=(12, 5))

# 计算每个评论的用词数。

num = [len(li) for li in li_2d]

# 统计用词最多的前15个评论。

length = 15

a = sns.barplot(x=np.arange(1, length + 1), y=sorted(num, reverse=True)[:length])

for container in a.containers:

    a.bar_label(container)
```

![[Pasted image 20230530120304.png]]

### 评论词汇长度分布统计

统计所有用户在评论时所使用的词汇数，绘制直方图。
``` python
plt.figure(figsize=(12, 5)) 
a = sns.histplot(num, bins=15) 
a.set_yscale("log") 
for container in a.containers: 
	a.bar_label(container)
```
![[Pasted image 20230530120603.png]]

## 绘制词云图

Python中，wordcloud模块提供了生成词云图的功能，我们可以使用该模块生成词云图。该模块并非Anaconda默 认的模块，需要独立安装后才能使用。

### 标准词云图

``` python
from wordcloud import WordCloud 
# 需要指定字体的位置，否则中文无法正常显示。 
wc = WordCloud(font_path=r"C:/Windows/Fonts/STFANGSO.ttf", width=800, height=600)
# WordCloud要求传递的词汇使用空格分开的字符串。 
join_words = " ".join(li_1d) 
img = wc.generate(join_words) 
plt.figure(figsize=(12, 10)) 
plt.imshow(img) 
plt.axis('off') # 将图像保存到本地。 
wc.to_file("wordcloud.png")
```

![[Pasted image 20230530122928.png]]

### 自定义背景

此外，我们还可以使用指定的图片作为背景，生成词云图。

``` python
wc = WordCloud(font_path=r"C:/Windows/Fonts/STFANGSO.ttf", mask=plt.imread("../imgs/map.jpg")) 
img = wc.generate(join_words) 
plt.figure(figsize=(12, 10))
plt.imshow(img) 
plt.axis('off')
```

![[Pasted image 20230530123251.png]]


## [[文本向量化]]

## 建立模型

### 构建训练集与测试集
我们需要将每条评论的词汇进行整理。目前，我们文本内容已经完成了分词处理，但词汇是以列表类型呈现的，为 了方便后续的向量化操作（文本向量化需要传递空格分开的字符串数组类型），我们将每条评论的词汇组合在一起，成为字符串类型，使用空格分隔。
``` python
def join(text_list): 
	return " ".join(text_list) 
data["comment"] = data["comment"].apply(join) 
data.sample(5)
```
![[Pasted image 20230530203920.png]]
然后，我们需要构造目标值，这里我们增加一个目标列（$y$）：
+ 好评 2
+ 中评 1
+ 差评 0

统计各个等级评论的数量
``` python
data["target"] = np.where(data["score"] >= 4.5, 2, np.where(data["score"] >= 3, 1, 0)) 
data["target"].value_counts()
```
![[Pasted image 20230530204115.png]]
得出：好评的数量非常多，容易让模型产生误差，所以，使用`sample`方法随机抽取数量与m相同的样本。
``` python
p = data[data["target"] == 2] 
m = data[data["target"] == 1] 
n = data[data["target"] == 0] 
p = p.sample(len(m))
data2 = pd.concat([p, m, n], axis=0) 
data2["target"].value_counts()
```
![[Pasted image 20230530204613.png]]
这样，我们就可以来对样本数据进行切分，构建训练集与测试集。
``` python
from sklearn.model_selection import train_test_split 
X = data2["comment"] 
y = data2["target"] 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) 
print("训练集样本数：", y_train.shape[0], "测试集样本数：", y_test.shape[0])
```
![[Pasted image 20230530204803.png]]

## 特征选择

### 特征维度

大家需要注意，到目前为止，数据集 还是文本类型，我们需要对其进行向量化操作。这里，我们使用 TfidfVectorizer类，在训练集上进行训练，然后分别对训练集与测试集实施转换。

``` python
# ngram_range：n元组模型的范围。 
# max_df，min_df：最大（小）的词频。词频过大或过小，对预测都没有太大帮助。 
vec = TfidfVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=1) 
X_train_tran = vec.fit_transform(X_train) 
X_test_tran = vec.transform(X_test) 
display(X_train_tran, X_test_tran)
```
![[Pasted image 20230530205156.png]]

### 方差分析
``` python
from sklearn.feature_selection import f_classif 
# 根据y进行分组，计算X中，每个特征的F值与P值。 
# F值越大，P值越小。则此时表示特征与目标值越有关系。 
f_classif(X_train_tran, y_train)
```
![[Pasted image 20230530205242.png]]

``` python
from sklearn.feature_selection import SelectKBest 
# tf-idf值不需要太多的精度，使用32位的浮点数表示足矣。 
X_train_tran = X_train_tran.astype(np.float32) 
X_test_tran = X_test_tran.astype(np.float32) 
# 定义特征选择器，用来选择最好的k个特征。 
selector = SelectKBest(f_classif, k=min(20000, X_train_tran.shape[1])) 
selector.fit(X_train_tran, y_train) 
# 对训练集与测试集进行转换（选择特征）。 
X_train_tran = selector.transform(X_train_tran) 
X_test_tran = selector.transform(X_test_tran) 
print(X_train_tran.shape, X_test_tran.shape)
```
![[Pasted image 20230530205332.png]]

## 朴素贝叶斯

``` python
from sklearn.metrics import classification_report 
from sklearn.naive_bayes import ComplementNB 
gnb = ComplementNB() 
gnb.fit(X_train_tran, y_train) 
y_hat = gnb.predict(X_test_tran) 
print(classification_report(y_test, y_hat))
```

![[Pasted image 20230530205428.png]]
