-----
## 模型说明
----
在线性回归中，如果存在多个自变量时，我们称该线性回归为多元线性回归。
$\widehat y = w_0+w_1*x_1+w_2*x_2+w_3*x_3+···+w_n*x_n$
+ $x_i$ ：第$i$ 个输入特征
+ $w_i$ ：第$i$ 特征的权重
+ $n$ ：特征的个数
+ $\widehat y$ ：预测值
-----
## 向量表示
-----
我们也可以使用向量的表示方式，设$\vec x$与$\vec w$为两个向量：

$\vec w = (w_{1}, w_{2}, w_{3}, \dots, w_{n})^T \\\vec x = (x_{1}, x_{2}, x_{3}, \dots, x_{n})^T$    

则回归方程可表示为：  
$\hat{y} = \sum_{j=1}^{n}(w_{j} * x_{j}) + w_0 \\= \vec w^T \cdot \vec x + w_0$   

我们可以进一步简化，为向量$\vec w$与$\vec x$各加入一个分量$w_0$与$x_0$，并且令：

$x_0 \equiv 1$    

于是，向量$\vec w$与$\vec x$就会变成：  

$\vec w = (w_0, w_{1}, w_{2}, w_{3}, \dots, w_{n})^T \\\vec x = (x_0, x_{1}, x_{2}, x_{3}, \dots, x_{n})^T$  

这样，就可以表示为：  

$\hat{y} = w_{0} * x_{0} + w_{1} * x_{1} + w_{2} * x_{2} + w_{3} * x_{3} + \dots + w_{n} * x_{n} \\= \sum_{j=0}^{n}(w_{j} * x_{j}) \\= \vec w^T \cdot \vec x$  

------
## 参数估计

### 误差与分布

接下来，我们来看一下线性回归模型中的误差。正如我们之前所提及的，线性回归中的自变量与因变量，是存在线性关系的。然而，这种关系并不是严格的函数映射关系，但是，我们构建的模型（方程）却是严格的函数映射关系，因此，对于每个样本来说，我们拟合的结果会与真实值之间存在一定的误差，我们可以将误差表示为：  

$\hat{y} ^ {(i)} = \vec{w}^T \cdot \vec{x}^{(i)}\\y ^ {(i)} = \hat{y}^{(i)} + \varepsilon^{(i)}$  

* $\varepsilon^{(i)}$：第$i$个样本真实值与预测值之间的误差（残差）。  

对于线性回归而言，具有一个前提假设：误差$\varepsilon$服从均值为0，方差为$\sigma^2$的正态分布。因此，根据正态分布的概率密度函数：  

$f(x) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x - \mu)^2}{2\sigma^2})$

则误差$\varepsilon$的分布为：  

$p(\varepsilon) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{\varepsilon^2}{2\sigma^2})$

因此，对于每一个样本的误差$\varepsilon^{(i)}$，其概率值为：  

$p(\varepsilon^{(i)};w) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2}) \\= \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)} - \vec{w}^T\vec{x}^{(i)})^2}{2\sigma^2})$

--------
### 极大似然估计
-----
极大似然估计（最大似然估计），是根据试验结果来估计未知参数的一种方式。其原则为：已经出现的，就是最有可能出现的，也就是令试验结果的概率值最大，来求解此时的未知参数值。

  

根据该原则，我们让所有误差出现的联合概率最大，则此时参数w的值，就是我们要求解的值，我们构建似然函数：  

$L(w) = \prod_{i=1}^{m}p(\varepsilon^{(i)};w) \\= \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)} - \vec{w}^T\vec{x}^{(i)})^2}{2\sigma^2})$  

* $m$：样本的数量。

### 对数似然函数

不过，累计乘积的方式不利于求解，我们这里使用对数似然函数，即在似然函数上取对数操作，这样就可以将累计乘积转换为累计求和的形式。  
$ln(L(w)) = ln\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)} - \vec{w}^T\vec{x}^{(i)})^2}{2\sigma^2}) \\= \sum_{i=1}^{m}ln\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)} - \vec{w}^T\vec{x}^{(i)})^2}{2\sigma^2}) \\= m * ln\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} * \frac{1}{2} * \sum_{i=1}^{m}(y^{(i)} - \vec{w}^T\vec{x}^{(i)})^2$

------
### 损失函数
上式中，前半部分都是常数，我们的目的是为了让对数似然函数值最大，故我们只需要让后半部分的值最小即可，因此，后半部分，就可以作为线性回归的损失函数。该函数是二次函数，具有唯一极小值。

$J(w) = \frac{1}{2} * \sum_{i=1}^{m}(y^{(i)} - \vec{w}^T\vec{x}^{(i)})^2 \qquad$

-----

### 损失函数化简
-----
根据矩阵与向量的求导公式，有：  

$\frac{\partial A\vec{x}}{\partial \vec{x}} = A^T \qquad\frac{\partial A\vec{x}}{\partial \vec{x}^T} = A \qquad\frac{\partial (\vec{x}^TA)}{\partial \vec{x}} = A \\\frac{\partial (\vec{x}^TA\vec{x})}{\partial \vec{x}} = (A^T + A)\vec{x}$  

特别的，如果$A = A ^ {T}$（A为对称矩阵），则：  

$\frac{\partial (\vec{x}^TA\vec{x})}{\partial \vec{x}} = 2A\vec{x}$ 

因此：  
$\frac{\partial}{\partial \vec{w}}(\frac{1}{2}(\vec{y}^T\vec{y} - \vec{y}^TX\vec{w} - \vec{w}^TX^T\vec{y} + \vec{w}^TX^TX\vec{w})) \\= \frac{1}{2}(-(\vec{y}^TX)^T - X^T\vec{y} + 2X^TX\vec{w}) \\= X^{T}X\vec{w} - X^T\vec{y}$
令导函数的值为0，则：  

$\vec{w} = (X^TX)^{-1}X^T\vec{y}$ 

* 矩阵$X^TX$必须是可逆的。

	上述代码实例如下
``` python
import numpy as np

import pandas as pd

#导入机器学习中的线性回归类
from sklearn.linear_model import LinearRegression

#导入切分数据集函数
from sklearn.model_selection import train_test_split

  

data = pd.read_csv("Advertising.csv", usecols=["TV","Radio","Newspaper","Sales"], header=0)

data.head()
X, y = data[["TV", "Radio", "Newspaper"]], data["Sales"]

#切分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

#线性回归实例化
lr = LinearRegression()

#拟合测试集
lr.fit(X_train, y_train)

#输出拟合后的权重及截距
print(lr.coef_)

print(lr.intercept_)

y_hat = lr.predict(X_test)

print(y_hat[:5])

print(y_test[:5])
```
