K-Means属于无监督学习，聚类的目的是根据样本数据 内部的特征，将数据划分为若干个类别，每个类别就是一个簇。结果为，使得同一个簇内的数据，相似 度较大，而不同簇内的数据，相似度较小。聚类也称为“无监督的分类”。其样本的相似性是根据距离来 度量的。

## K-Means算法

### 算法步骤

K-Mean算法，即 均值算法，是最常见的一种聚类算法。顾名思义，该算法会将数据集分为 个簇， 每个簇使用簇内所有样本的均值来表示，我们将该均值称为“质心”。具体步骤如下： 
1. 从样本中选择 个点作为初始质心。 
2. 计算每个样本到各个质心的距离，将样本划分到距离最近的质心所对应的簇中。 
3. 计算每个簇内所有样本的均值，并使用该均值更新簇的质心。 
4. 重复步骤2与3，直到达到以下条件之一结束： 
	 + 质心的位置变化小于指定的阈值。 
	 + 达到最大迭代次数。
停止条件：
1. 设置迭代次数。
2. 质心的位置变化小于指定的阈值。

SSE衡量的标准是模型的簇数量一样，当模型之间的簇的数量不一样时，SSE的衡量没有意义。

### 优化目标

KMeans算法的目标就是选择合适的质心，使得在每个簇内，样本距离质心的距离尽可能的小。这样就 可以保证簇内样本具有较高的相似性。我们可以使用最小化簇内误差平方和（within-cluster sum-ofsquares ）来作为优化算法的量化目标（目标函数），簇内误差平方和也称为簇惯性（inertia）。
$$
SSE=\textstyle\sum_{i=1}^k\sum_{j=1}^{m_i}(||x_j-\mu_i||^2)
$$
+ $k$：簇的数量
+ $m_i$：第i个簇含有的样本数量。
+ $\mu_i$：第$i$个簇的质心
+ $||x_j+\mu_i||$：第$i$个簇中，每个样本$x_j$与质心$\mu_i$的距离。


## K-Means算法优缺点

优点如下：
1. 容易实现和理解，是聚类任务中的常用选择
2. 计算效率高，可以处理高维大型数据集。
3. 各个领域都可以使用

缺点：
1. 必须事先指定K值，如果K值选择不当，聚类效果可能不好。
2. 聚类容易收到初始质心的影响，位置如果不好的话可能会收敛到局部最小值。
3. 适用于凸形数据分布，对于条形或者形状不规则的数据，效果较差。


