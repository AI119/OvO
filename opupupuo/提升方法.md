<font color="FFA488" size="6">提升方法集成的是弱模型，不太容易产生过拟合，但是如果产生过拟合，可能是评估器太多，减少一些评估器即可</font>

通常使用加法模型，将每个基本预测结果相加，也可能给每个基本模型加一个权重。

使用同一个训练集，每个样本开始的权重相同，通过不断调整预测错误的样本的权重，不断迭代，直到找出一个最佳的训练模型。

模型的权重取决于模型预测的正确率，正确率高的模型权重也就变高。

## AdaBoost

AdaBoost（Adaptive Boosting——自适应提升），是一种集成方法，应用提升方法的思想。算法将多个弱评估器 的输出结合起来创建一个强评估器，从而提高整体预测精度。

这个算法相当于一个深度只有1的决策树，通过不断调整样本的权重，得出最佳的拟合效果。

### 工作原理

AdaBoost的工作原理是： 
+ 在迭代过程中，调整错误分类样本的权重。 
	+ 如果样本预测正确，则降低权重。 
	+ 如果样本预测错误，则提高权重。 
+ 在更新后的权重上训练一个新的弱分类器，以专注于难以分类的样本。 
	+ 越难区分的样本在训练过程中会变得越重要。

权重越高，样本预测错的代价越高

![[Pasted image 20230607103430.png]]

### 提升预测

Adaboost算法含有两个权重，一个是样本的权重，一个是基本评估器的权重。

算法给误差率较小的基本评估器以较大的权重，给误差率较大的基本评估器以较小的权重。最终基本评估器的预测 结果进行线性加权组合，得到最终的预测。$$
\hat{y}=F_n(x)=\sum_{i=1}^n\alpha_if_i(x)
$$
+ $F_n(x)$：是前n个评估器累计输出结果
+ $f_i(x)$：每个评估器输出结果
+ $\alpha_i$：每个基本评估器的权重
+ $n$：基本评估器的数量

对于分类任务，则在最终的结果上进行sign函数的转换即可：$$
\hat{y}=sign(F_n(x))=sign(\sum_{i=1}^n\alpha_if_i(x))
$$
![[Pasted image 20230607104431.png]]

### AdaBoost算法步骤
---------

1. 初始化每个样本的权重$w$，使所有样本的初始权重相同，并且权重之和为1，即：$$
w_1=(\frac{1}{m},\frac{1}{m},\ldots,\frac{1}{m})^T
$$
2. 在第$k$轮迭代中，使用具有权重$W_k$的样本训练基本评估器$f_k(x)$

3. 使用基本学习器$f_k(x)$预测样本输出值$\hat{y}$

4. 计算含有权重的错误率
$$
\epsilon_k=w_k\cdot(y\neq\hat{y})
$$
注意，这里的y是向量，是一个数组，$w_k$点乘括号内的元素。
注意，预测错为True(数值为1)，所以$\epsilon_k$越高，也就是错误率越高。

![[Pasted image 20230607105810.png]]

5. 计算第$k$轮的基本评估器$f_k(x)$的权重系数$$
\alpha_k=0.5*log\frac{1-\epsilon_k}{\epsilon_k}
$$
+ $\alpha_k>0$
错误率高于0.5的是弱智模型。

6. 更新样本权重：$$
w_k=w_k*e^{-\alpha_k*y*\hat{y}}
$$
+ 对于预测正确的样本，降低样本权重值，否则提升样本权重值

7. .对权重 进行归一化，使其和为1：$$w_{k}=\frac{w_{j}}{\sum_{i}w_{i}}$$
8. 构建基本评估器的线性组合：$$F_k(x)=\sum_{i=1}^k\alpha_if_i(x)$$
+ 该结果为迭代到第$k$轮的预测结果

9. 重复步骤2~8，共$n$次，获得最终的评估器：$$\hat{y}=sign(F_n(x))=sign(\sum_{i=1}^n\alpha_if_i(x))$$
算法示例在下列文档中的3-10页
![[提升方法.pdf]]






## 泰勒公式

泰勒公式用来将一个函数表示为一个无限级数的形式，这个级数的每一项都是函数在给定点处的导数。通过泰勒公 式，可用于求解函数的近似值。泰勒公式的形式如下：$$
f(x)=\sum_{n=0}^{\infty}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
$$
+ $f^{(n)}(x_0)$：函数$f(x)$在$x=x_0$处的$n$阶导数
例如，当函数$f(x)$在点$x_0$处可导时，我们可以将函数$f(x)$使用一阶泰勒在点$x_0$展开：$$
f(x)\approx f(x_0)+f'(x_0)(x-x_0)
$$
二阶泰勒在点$x_0$处展开：$$
f(x)\approx f(x_0)+f'(x_0)(x-x_0)+0.5*f''(x_0)(x-x_0)^2
$$



$$
l(y_i,F_{m-1}(x_i)+h_m(x_i))\approx l(y_i,F_{m-1}(x_i))+h_m(x_i)\bigg[\frac{\partial l(y_i,F(x_i))}{\partial F(x_i)}\bigg]_{F=F_{m-1}}
$$

**拟合**

令上式中的偏导值为$g_i$，则：$$\begin{aligned}
h_m& \approx\arg\min\limits_{h}\sum\limits_{i=1}^n l(y_i,F_{m-1}(x_i))+h(x_i)g_i  \\
&\approx\arg\min\limits_h\sum\limits_{i=1}^n h(x_i)g_i
\end{aligned}$$

对于$h(x_i)$所拟合的方向，应该为拟合样本负梯度，能够使得函数值最小。

GBDT内部使用的是<big>回归树</big>，，对于分类任务，与回归的训练方式基本是相同的，只是在最后 输出$F_M(x_i)$（连续值）的基础上，通过相关函数完成转换， 过程与逻辑回归算法相似。
+ 对于二分类任务，使用sigmoid函数。 
+ 对于多分类任务，使用softmax函数。







